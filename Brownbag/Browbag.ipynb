{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amir-jafari/NLP/blob/master/Brownbag/Browbag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String\n",
    "\n",
    "A string type object is a sequence of characters. Each string is stored in computer array of characters. You can access individual characters by using indices in square\n",
    "brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "D\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "String= \"ABCD\"\n",
    "print(String[0])\n",
    "print(String[1])\n",
    "print(String[-1])\n",
    "print(String[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on string methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abcd. abcd.\n",
      "2\n",
      "False\n",
      "2\n",
      "['Abcd', ' Abcd', '']\n"
     ]
    }
   ],
   "source": [
    "S4 = 'abcd. Abcd.'\n",
    "print(S4.capitalize())\n",
    "print(S4.count('c'))\n",
    "print(S4.isalnum())\n",
    "print(S4.index('c'))\n",
    "print(S4.split(sep='.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Write a function that counts the numbers of a particular letter in a string. For example count the number of letter a in a word **Bannana**. \n",
    "Note: Compare your function with a count method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def COUNT(Word, let):\n",
    "    word = Word\n",
    "    count = 0\n",
    "    for letter in word:\n",
    "        if letter == let:\n",
    "            count = count + 1\n",
    "    return print (count)\n",
    "\n",
    "COUNT('Bannana', 'a')\n",
    "# --------------------------------\n",
    "print('Bannana'.count('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use collections packages to create Counter Objects and  defaultdict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'c': 3, 'a': 2, 'b': 1, 'd': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "list_char = ['a', 'b', 'c', 'd', 'a' , 'c', 'c']\n",
    "print(Counter(list_char))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "For example, letâ€™s say you want the count of each name in a list of names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "defaultdict(<class 'int'>, {'John': 3, 'Julie': 1, 'Jack': 3, 'Ann': 1, 'Mike': 1, 'Jen': 3, 'Smith': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "nums = defaultdict(int)\n",
    "nums['one'] = 1\n",
    "nums['two'] = 2\n",
    "print(nums['three'])\n",
    "\n",
    "count = defaultdict(int)\n",
    "names = \"John Julie Jack Ann Mike John John Jack Jack Jen Smith Jen Jen\"\n",
    "list = names.split(sep=' ')\n",
    "for names in list:\n",
    "    count[names] +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Write a function that reads the Story text and finds the strings in the curly brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Story = \"\"\"\n",
    "Once upon a time, deep in an ancient jungle,there lived a {animal}. This {animal} liked to eat {food}, but the jungle had \n",
    "very little {food} to offer. One day, an explorer found the {animal} and discovered it liked {food}. The explorer took the\n",
    "{animal} back to {city}, where it could eat as much {food} as it wanted. However, the {animal} became homesick, so the\n",
    "explorer brought it back to the jungle, leaving a large supply of {food}.\n",
    "\n",
    "The End\n",
    "\"\"\"\n",
    "def getKeys(formatString):\n",
    "    ##\n",
    "    #To DO\n",
    "    ##\n",
    "    return keyList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle']\n",
      "['Once', 'upon', 'a', 'time', ',', 'deep', 'in', 'an', 'ancient', 'jungle', ',', 'there', 'lived', 'a', '{', 'animal', '}', '.', 'This', '{', 'animal', '}', 'liked', 'to', 'eat', '{', 'food', '}', ',', 'but', 'the', 'jungle', 'had', 'very', 'little', '{', 'food', '}', 'to', 'offer', '.', 'One', 'day', ',', 'an', 'explorer', 'found', 'the', '{', 'animal', '}', 'and', 'discovered', 'it', 'liked', '{', 'food', '}', '.', 'The', 'explorer', 'took', 'the', '{', 'animal', '}', 'back', 'to', '{', 'city', '}', ',', 'where', 'it', 'could', 'eat', 'as', 'much', '{', 'food', '}', 'as', 'it', 'wanted', '.', 'However', ',', 'the', '{', 'animal', '}', 'became', 'homesick', ',', 'so', 'the', 'explorer', 'brought', 'it', 'back', 'to', 'the', 'jungle', ',', 'leaving', 'a', 'large', 'supply', 'of', '{', 'food', '}', '.', 'The', 'End']\n"
     ]
    }
   ],
   "source": [
    "import  nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(Story)\n",
    "print(type(tokens))\n",
    "print(tokens[:10])\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(120*'-')\n",
    "print( [lancaster.stem(t) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwprds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "0.735240435097661\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "print(content_fraction(nltk.corpus.reuters.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', 'would', 'get', 'home', '.']\n",
      "Counter({'the': 3, 'faster': 2, 'harry': 2, 'got': 1, 'to': 1, 'store': 1, ',': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"The faster Harry got to the store, the faster Harry  would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "print(tokens)\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4), ('faster', 3), ('harry', 2), (',', 2)]\n",
      "0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "sentence = \"The faster Harry got to the store, the faster Harry \" \\\n",
    "           \"the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "bag_of_words = Counter(tokens)\n",
    "print(bag_of_words.most_common(4))\n",
    "\n",
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "tf = times_harry_appears / num_unique_words\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "33\n",
      "18\n",
      "OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('is', 0), ('jill', 0), ('not', 0), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])\n",
      "          ,         .       and        as    faster       get       got  \\\n",
      "0  0.055556  0.055556  0.055556  0.000000  0.166667  0.055556  0.055556   \n",
      "1  0.000000  0.055556  0.055556  0.000000  0.055556  0.000000  0.000000   \n",
      "2  0.000000  0.055556  0.000000  0.111111  0.000000  0.000000  0.000000   \n",
      "\n",
      "      hairy     harry      home        is      jill       not     store  \\\n",
      "0  0.000000  0.111111  0.055556  0.000000  0.000000  0.000000  0.055556   \n",
      "1  0.055556  0.055556  0.000000  0.055556  0.055556  0.000000  0.000000   \n",
      "2  0.055556  0.055556  0.000000  0.055556  0.055556  0.055556  0.000000   \n",
      "\n",
      "       than       the        to     would  \n",
      "0  0.000000  0.166667  0.055556  0.055556  \n",
      "1  0.055556  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import OrderedDict, Counter\n",
    "import copy\n",
    "import pandas as pd\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "docs = [\"The faster Harry got to the store, the faster and faster \"\n",
    "        \"Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "print(len(doc_tokens[0]))\n",
    "all_doc_tokens = sum(doc_tokens, []) ; print(len(all_doc_tokens))\n",
    "lexicon = sorted(set(all_doc_tokens)); print(len(lexicon))\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon); print(zero_vector)\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "df = pd.DataFrame(doc_vectors);print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "data\n",
      "computers\n",
      "algorithms\n",
      "learning\n",
      "machine\n",
      "science\n",
      "java\n",
      "programming\n",
      "structures\n",
      "home\n",
      " \n",
      "Concept 1:\n",
      "fun\n",
      "family\n",
      "home\n",
      "kids\n",
      "money\n",
      "energy\n",
      "food\n",
      "games\n",
      "health\n",
      "love\n",
      " \n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Data Science Machine Learning\"\n",
    "doc2 = \"Money fun Family Kids home\"\n",
    "doc3 = \"Programming Java Data Structures\"\n",
    "doc4 = \"Love food health games energy fun\"\n",
    "doc5 = \"Algorithms Data Computers\"\n",
    "\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X =vectorizer.fit_transform(doc_complete)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2,n_iter=100)\n",
    "lsa.fit(X)\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Logsitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  60.0\n",
      "\n",
      "Naive Bayes Confusion Matrix -> \n",
      "  [[ 4 12]\n",
      " [ 0 14]]\n",
      "\n",
      "Naive Bayes Cohen Kappa Score ->  0.23728813559322037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "np.random.seed(500)\n",
    "Corpus = pd.read_csv(\"https://raw.githubusercontent.com/amir-jafari/NLP/master/Brownbag/data.csv\", encoding='latin-1').head(100)\n",
    "Corpus['text'].dropna(inplace=True)\n",
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n",
    "Corpus['text'] = [word_tokenize(entry) for entry in Corpus['text']]\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(Corpus['text']):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    Corpus.loc[index, 'text_final'] = str(Final_words)\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'], Corpus['label'], test_size=0.3)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(Test_Y, predictions_NB) * 100)\n",
    "print()\n",
    "print(\"Naive Bayes Confusion Matrix -> \\n \", confusion_matrix(Test_Y, predictions_NB))\n",
    "print()\n",
    "print(\"Naive Bayes Cohen Kappa Score -> \", cohen_kappa_score(Test_Y, predictions_NB))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
